{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic dynamic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../backend/data/dynamic_dataset'\n",
    "DATA_PATH = '../backend/dynamic_signs/frames'\n",
    "CSV_PATH = '../out.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1710765234.026867   35430 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1710765234.028148   36530 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 Mesa 21.2.6), renderer: Mesa Intel(R) UHD Graphics 620 (WHL GT2)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msign\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlandmark_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mMediaPiper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MediaPiper\n\u001b[1;32m      4\u001b[0m mp \u001b[38;5;241m=\u001b[39m MediaPiper()\n\u001b[0;32m----> 5\u001b[0m gestures \u001b[38;5;241m=\u001b[39m \u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_dynamic_gestures_from_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/6th_Semester/Bachelor/Project/backend/sign/training/landmark_extraction/MediaPiper.py:157\u001b[0m, in \u001b[0;36mMediaPiper.process_dynamic_gestures_from_folder\u001b[0;34m(self, base_path)\u001b[0m\n\u001b[1;32m    155\u001b[0m cur: GestureSequence \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    156\u001b[0m labelRes: \u001b[38;5;28mlist\u001b[39m[GestureSequence] \u001b[38;5;241m=\u001b[39m [cur]\n\u001b[0;32m--> 157\u001b[0m prev_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__extract_prefix(\u001b[43mfiles\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m    159\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__extract_prefix(image)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#Load DATA\n",
    "from sign.training.landmark_extraction.MediaPiper import MediaPiper\n",
    "\n",
    "mp = MediaPiper()\n",
    "gestures = mp.process_dynamic_gestures_from_folder(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(gesture.label, f\"Length: {len(gesture.results)}\") for gesture in gestures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sign.trajectory import TrajectoryBuilder, trajectory\n",
    "\n",
    "bob = TrajectoryBuilder()\n",
    "\n",
    "gesture_trajector_map: dict[str, list[trajectory]] = {}\n",
    "\n",
    "for gesture in gestures:\n",
    "    gesture_trajector_map[gesture.label] = []\n",
    "    for sequence in gesture.results:\n",
    "        new_sequence = []\n",
    "        for image_mp_res in sequence:\n",
    "            hand_landmarks = np.array(image_mp_res.multi_hand_landmarks)\n",
    "            new_sequence.append(hand_landmarks)\n",
    "        new_sequence = bob.extract_keyframes_sample(new_sequence)\n",
    "        sequence_as_np_array = np.array(new_sequence)\n",
    "        sequence_trajectory = bob.make_trajectory(sequence_as_np_array)\n",
    "\n",
    "        gesture_trajector_map[gesture.label].append(sequence_trajectory)\n",
    "\n",
    "\n",
    "print(gesture_trajector_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for k,gesture in gesture_trajector_map.items():\n",
    "    min = len(gesture[0].directions)\n",
    "    for trajectory in gesture[1:]:\n",
    "        length = len(trajectory.directions)\n",
    "        if min > length:\n",
    "            min = length\n",
    "    for trajectory in gesture:\n",
    "        if len(trajectory.directions) > min:\n",
    "            for i in range(len(trajectory.directions) - min):\n",
    "                trajectory.directions.pop()\n",
    "\n",
    "\n",
    "    for trajector in gesture:\n",
    "        xyz  = []\n",
    "        for te in trajector.directions:\n",
    "            xyz.append(te.x.value)\n",
    "            xyz.append(te.y.value)\n",
    "            xyz.append(te.z.value)\n",
    "        X.append(np.array(xyz))\n",
    "        y.append(k)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from joblib import dump\n",
    "    dump(model, 'dynamic_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Gesture Recognizer but with landmarks :eyes:\n",
    "\n",
    "So the idea is that we will still use the \"basic\" trajectory, all landmarks reduced to mean x,y,z values.\n",
    "But, next to this trajectory we will attach preprocessed landmarks for each of the frames.\n",
    "- That is, we will only attach preprocessed landmarks corresponding to the chosen key frames.\n",
    "- The preprocessing step is EXACTLY the same as for static images.\n",
    "  - Which is also why the 'Z' values have been cut in the following steps. No other reason than our methods already in use for preprocssing do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "csv_reader.extract_landmarks() missing 1 required positional argument: 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_training_data_and_labels_from_dynamic_gesture_csv\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mlist\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray], \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m extract_training_data_and_labels_from_dynamic_gesture_map(reader\u001b[38;5;241m.\u001b[39mextract_landmarks())\n\u001b[0;32m---> 72\u001b[0m \u001b[43mextract_training_data_and_labels_from_dynamic_gesture_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 70\u001b[0m, in \u001b[0;36mextract_training_data_and_labels_from_dynamic_gesture_csv\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_training_data_and_labels_from_dynamic_gesture_csv\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mlist\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray], \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m extract_training_data_and_labels_from_dynamic_gesture_map(\u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_landmarks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: csv_reader.extract_landmarks() missing 1 required positional argument: 'path'"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "import random\n",
    "import numpy as np\n",
    "from sign.training.landmark_extraction.MediaPiper import DynamicGesture\n",
    "from sign.trajectory import TrajectoryBuilder, trajectory\n",
    "from sign.landmarks import pre_process_landmark, calc_landmark_list\n",
    "from dynamic_signs.csv_reader import csv_reader\n",
    "\n",
    "def training_trajectories_with_normalized_landmarks(gestures:list[DynamicGesture]) -> dict[str, list[np.ndarray]]:\n",
    "    bob = TrajectoryBuilder()\n",
    "\n",
    "    gesture_trajector_map: dict[str, list[np.ndarray]] = {}\n",
    "\n",
    "    for gesture in gestures:\n",
    "        gesture_trajector_map[gesture.label] = []\n",
    "        for sequence in gesture.results:\n",
    "            new_sequence : list[Tuple] = []\n",
    "            #flatmarks: list[np.ndarray] = []\n",
    "            for image_mp_res in sequence:\n",
    "                if image_mp_res.multi_hand_landmarks is not None:\n",
    "                    hand_landmarks = np.array(image_mp_res.multi_hand_landmarks)\n",
    "\n",
    "                    normalized_landmarks_for_image = pre_process_landmark(\n",
    "                        calc_landmark_list(image_mp_res.multi_hand_landmarks))\n",
    "                    flatmark = np.array(normalized_landmarks_for_image)\n",
    "\n",
    "                    new_sequence.append((hand_landmarks, flatmark))\n",
    "\n",
    "            #new_sequence = bob.extract_keyframes_sample(new_sequence)\n",
    "            random.seed(42)\n",
    "            res = [new_sequence[0]]\n",
    "            res.extend(random.sample(new_sequence[1:-1], bob.target_len - 2))\n",
    "            res.append(new_sequence[-1])\n",
    "            \n",
    "            \n",
    "            hand_landmark_seq = [ hand_landmark for hand_landmark, _ in res ]\n",
    "            flatmarks = [ flatmark for _, flatmark in res ] #without Z values\n",
    "            sequence_as_np_array = np.array(hand_landmark_seq)\n",
    "            sequence_trajectory = bob.make_trajectory(sequence_as_np_array)\n",
    "\n",
    "            xyz = []\n",
    "            for trajectory_elm in sequence_trajectory.directions:\n",
    "                xyz.append(trajectory_elm.x.value)\n",
    "                xyz.append(trajectory_elm.y.value)\n",
    "                xyz.append(trajectory_elm.z.value)\n",
    "            xyz = np.array(xyz)\n",
    "\n",
    "            \n",
    "            for flat_landmark in flatmarks:\n",
    "                xyz = np.append(xyz, flat_landmark)\n",
    "\n",
    "            gesture_trajector_map[gesture.label].append(xyz)\n",
    "\n",
    "    return gesture_trajector_map\n",
    "\n",
    "\n",
    "def extract_training_data_and_labels_from_dynamic_gesture_map(gesture_map: dict[str, list[np.ndarray]]) -> Tuple[list[np.ndarray], list[str]]:\n",
    "    trajectories_and_landmarks: list[np.ndarray] = []\n",
    "    labels : list[str] = []\n",
    "    for label, label_data in gesture_map.items():\n",
    "        for data in label_data:\n",
    "            labels.append(label)\n",
    "            trajectories_and_landmarks.append(data)\n",
    "\n",
    "    return trajectories_and_landmarks, labels\n",
    "\n",
    "reader = csv_reader()\n",
    "\n",
    "def extract_training_data_and_labels_from_dynamic_gesture_csv() -> Tuple[list[np.ndarray], list[str]]:\n",
    "    return extract_training_data_and_labels_from_dynamic_gesture_map(reader.extract_landmarks())\n",
    "\n",
    "extract_training_data_and_labels_from_dynamic_gesture_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = training_trajectories_with_normalized_landmarks(gestures)\n",
    "train_data, train_labels  = extract_training_data_and_labels_from_dynamic_gesture_map(map)\n",
    "len(train_data), len(train_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model_logistic = LogisticRegression()\n",
    "model_logistic.fit(train_data, train_labels)\n",
    "\n",
    "model_svm = make_pipeline(StandardScaler(),\n",
    "                          SVC(kernel=\"poly\", degree=6, coef0=1))\n",
    "model_svm.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_tuple_hand_landmarks_normalized(tuple_list:list[Tuple]):\n",
    "    random.seed(42)\n",
    "    res = [tuple_list[0]]\n",
    "    res.extend(random.sample(tuple_list[1:-1], bob.target_len - 2))\n",
    "    res.append(tuple_list[-1])\n",
    "    \n",
    "    \n",
    "    hand_landmark_seq = [ hand_landmark for hand_landmark, _ in res ]\n",
    "    flatmarks = [ flatmark for _, flatmark in res ] #without Z values\n",
    "    return hand_landmark_seq, flatmarks\n",
    "\n",
    "def mp_landmarks_to_dynamic_prediction_ready(landmarks):\n",
    "    tuple_list = []\n",
    "    for landmark in landmarks:\n",
    "        pre_processed = pre_process_landmark(calc_landmark_list(landmark))\n",
    "        hand_landmark = np.array(landmark)\n",
    "        tuple_list.append((hand_landmark, pre_processed))\n",
    "    \n",
    "    key_frame_tuple = sample_tuple_hand_landmarks_normalized(tuple_list)\n",
    "\n",
    "    flatmarks = key_frame_tuple[1]\n",
    "    sequence_as_np_array = np.array(key_frame_tuple[0])\n",
    "    trajectory = bob.make_trajectory(sequence_as_np_array)\n",
    "\n",
    "    xyz = []\n",
    "    for trajectory_elm in trajectory.directions:\n",
    "        xyz.append(trajectory_elm.x.value)\n",
    "        xyz.append(trajectory_elm.y.value)\n",
    "        xyz.append(trajectory_elm.z.value)\n",
    "    xyz = np.array(xyz)\n",
    "\n",
    "    \n",
    "    for flat_landmark in flatmarks:\n",
    "        xyz = np.append(xyz, flat_landmark)\n",
    "\n",
    "    return xyz\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test on a single Z\n",
    "TEST_PATH = '../backend/dynamic_signs/test'\n",
    "\n",
    "test_data = mp.process_dynamic_gestures_from_folder(TEST_PATH)\n",
    "#res = list(filter(lambda x : x.label == 'Z', res))\n",
    "labels_test = [result.label for result in test_data]\n",
    "labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_material = []\n",
    "for dynamic_gesture in test_data:\n",
    "    for sequence in dynamic_gesture.results:\n",
    "        hand_landmarks_for_seq = [mp_result.multi_hand_landmarks for mp_result in sequence \n",
    "                                  if mp_result.multi_hand_landmarks is not None]\n",
    "        prediction_material.append(\n",
    "            mp_landmarks_to_dynamic_prediction_ready(hand_landmarks_for_seq)\n",
    "        )\n",
    "\n",
    "print(\"Logistis predictions: \", model_logistic.predict(prediction_material))\n",
    "print(\"Support vector machine: \", model_svm.predict(prediction_material))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metrics for Dynamic Gesture recogniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "cr = classification_report(model_svm.predict(prediction_material), labels_test)\n",
    "print(cr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
