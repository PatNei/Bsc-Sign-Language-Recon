{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic dynamic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../backend/data/dynamic_dataset'\n",
    "DATA_PATH = '../backend/dynamic_signs/frames'\n",
    "CSV_PATH = '../out.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load DATA\n",
    "from sign.training.landmark_extraction.MediaPiper import MediaPiper\n",
    "\n",
    "mp = MediaPiper()\n",
    "gestures = mp.process_dynamic_gestures_from_folder(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(gesture.label, f\"Length: {len(gesture.results)}\") for gesture in gestures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ðŸ”¥ TrajectoryBuilder is now running in BERTRAM_MODE ðŸ”¥ðŸ”¥\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sign.trajectory import TrajectoryBuilder, trajectory\n",
    "\n",
    "bob = TrajectoryBuilder()\n",
    "\n",
    "gesture_trajector_map: dict[str, list[trajectory]] = {}\n",
    "\n",
    "for gesture in gestures:\n",
    "    gesture_trajector_map[gesture.label] = []\n",
    "    for sequence in gesture.results:\n",
    "        new_sequence = []\n",
    "        for image_mp_res in sequence:\n",
    "            hand_landmarks = np.array(image_mp_res.multi_hand_landmarks)\n",
    "            new_sequence.append(hand_landmarks)\n",
    "        new_sequence = bob.extract_keyframes_sample(new_sequence)\n",
    "        sequence_as_np_array = np.array(new_sequence)\n",
    "        sequence_trajectory = bob.make_trajectory(sequence_as_np_array)\n",
    "\n",
    "        gesture_trajector_map[gesture.label].append(sequence_trajectory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for k,gesture in gesture_trajector_map.items():\n",
    "    min = len(gesture[0].directions)\n",
    "    for trajectory in gesture[1:]:\n",
    "        length = len(trajectory.directions)\n",
    "        if min > length:\n",
    "            min = length\n",
    "    for trajectory in gesture:\n",
    "        if len(trajectory.directions) > min:\n",
    "            for i in range(len(trajectory.directions) - min):\n",
    "                trajectory.directions.pop()\n",
    "\n",
    "\n",
    "    for trajector in gesture:\n",
    "        xyz  = []\n",
    "        for te in trajector.directions:\n",
    "            xyz.append(te.x.value)\n",
    "            xyz.append(te.y.value)\n",
    "            xyz.append(te.z.value)\n",
    "        X.append(np.array(xyz))\n",
    "        y.append(k)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0xffff963547d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/repositories/Bsc-Sign-Language-Recon/backend/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression, LogisticRegression\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/repositories/Bsc-Sign-Language-Recon/backend/.venv/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/repositories/Bsc-Sign-Language-Recon/backend/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1201\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1199\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[0;32m-> 1201\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1209\u001b[0m check_classification_targets(y)\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[0;32m/mnt/repositories/Bsc-Sign-Language-Recon/backend/.venv/lib/python3.11/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/mnt/repositories/Bsc-Sign-Language-Recon/backend/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:1263\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1258\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1260\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1261\u001b[0m     )\n\u001b[0;32m-> 1263\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1279\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1281\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m/mnt/repositories/Bsc-Sign-Language-Recon/backend/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:1035\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1029\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1030\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1031\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1032\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1033\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1034\u001b[0m             )\n\u001b[0;32m-> 1035\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1039\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1041\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from joblib import dump\n",
    "    dump(model, 'dynamic_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Gesture Recognizer but with landmarks :eyes:\n",
    "\n",
    "So the idea is that we will still use the \"basic\" trajectory, all landmarks reduced to mean x,y,z values.\n",
    "But, next to this trajectory we will attach preprocessed landmarks for each of the frames.\n",
    "- That is, we will only attach preprocessed landmarks corresponding to the chosen key frames.\n",
    "- The preprocessing step is EXACTLY the same as for static images.\n",
    "  - Which is also why the 'Z' values have been cut in the following steps. No other reason than our methods already in use for preprocssing do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ðŸ”¥ TrajectoryBuilder is now running in BERTRAM_MODE ðŸ”¥ðŸ”¥\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(708, 708)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "import random\n",
    "import numpy as np\n",
    "from sign.training.landmark_extraction.MediaPiper import DynamicGesture\n",
    "from sign.trajectory import TrajectoryBuilder, trajectory\n",
    "from sign.landmarks import pre_process_landmark, calc_landmark_list\n",
    "from dynamic_signs.csv_reader import csv_reader\n",
    "\n",
    "def training_trajectories_with_normalized_landmarks(gestures:list[DynamicGesture]) -> dict[str, list[np.ndarray]]:\n",
    "\n",
    "    gesture_trajector_map: dict[str, list[np.ndarray]] = {}\n",
    "    bob = TrajectoryBuilder()\n",
    "\n",
    "    for gesture in gestures:\n",
    "        gesture_trajector_map[gesture.label] = []\n",
    "        for sequence in gesture.results:\n",
    "            new_sequence : list[Tuple] = []\n",
    "            #flatmarks: list[np.ndarray] = []\n",
    "            for image_mp_res in sequence:\n",
    "                if image_mp_res.multi_hand_landmarks is not None:\n",
    "                    hand_landmarks = np.array(image_mp_res.multi_hand_landmarks)\n",
    "\n",
    "                    normalized_landmarks_for_image = pre_process_landmark(\n",
    "                        calc_landmark_list(image_mp_res.multi_hand_landmarks))\n",
    "                    flatmark = np.array(normalized_landmarks_for_image)\n",
    "\n",
    "                    new_sequence.append((hand_landmarks, flatmark))\n",
    "\n",
    "            #new_sequence = bob.extract_keyframes_sample(new_sequence)\n",
    "            random.seed(42)\n",
    "            res = [new_sequence[0]]\n",
    "            res.extend(random.sample(new_sequence[1:-1], bob.target_len - 2))\n",
    "            res.append(new_sequence[-1])\n",
    "            \n",
    "            \n",
    "            hand_landmark_seq = [ hand_landmark for hand_landmark, _ in res ]\n",
    "            flatmarks = [ flatmark for _, flatmark in res ] #without Z values\n",
    "            sequence_as_np_array = np.array(hand_landmark_seq)\n",
    "            sequence_trajectory = bob.make_trajectory(sequence_as_np_array)\n",
    "\n",
    "            xyz = []\n",
    "            for trajectory_elm in sequence_trajectory.directions:\n",
    "                xyz.append(trajectory_elm.x.value)\n",
    "                xyz.append(trajectory_elm.y.value)\n",
    "                xyz.append(trajectory_elm.z.value)\n",
    "            xyz = np.array(xyz)\n",
    "\n",
    "            \n",
    "            for flat_landmark in flatmarks:\n",
    "                xyz = np.append(xyz, flat_landmark)\n",
    "\n",
    "            gesture_trajector_map[gesture.label].append(xyz)\n",
    "\n",
    "    return gesture_trajector_map\n",
    "\n",
    "\n",
    "def extract_training_data_and_labels_from_dynamic_gesture_map(gesture_map: dict[str, list[np.ndarray]]) -> Tuple[list[np.ndarray], list[str]]:\n",
    "    trajectories_and_landmarks: list[np.ndarray] = []\n",
    "    labels : list[str] = []\n",
    "    for label, label_data in gesture_map.items():\n",
    "        for data in label_data:\n",
    "            labels.append(label)\n",
    "            trajectories_and_landmarks.append(data)\n",
    "\n",
    "    return trajectories_and_landmarks, labels\n",
    "\n",
    "reader = csv_reader()\n",
    "\n",
    "\n",
    "\n",
    "def prune_training_data_and_labels_from_dynamic_gesture_csv(input: dict[str, dict[int, list[float]]]) -> dict[str, list[np.ndarray]]:\n",
    "    target_length = 24*3*21\n",
    "    bob = TrajectoryBuilder(target_len=target_length)\n",
    "    res: dict[str, list[np.ndarray]] = {}\n",
    "    for label, videos in input.items():\n",
    "        for _, frames in videos.items():\n",
    "            existing = res.get(label)\n",
    "            if len(frames) < target_length:\n",
    "                frames = bob.pad_sequences_of_landmarks(frames)\n",
    "            else: \n",
    "                frames = bob.extract_keyframes_sample(frames)\n",
    "                \n",
    "            if existing is not None:\n",
    "                existing.append(frames)\n",
    "            else:\n",
    "                res[label] = [frames]\n",
    "                \n",
    "    return res\n",
    "    \n",
    "def extract_training_data_and_labels_from_dynamic_gesture_csv() -> Tuple[list[np.ndarray], list[str]]:\n",
    "    unpruned = reader.extract_landmarks(\"../backend/out.csv\")\n",
    "    pruned = prune_training_data_and_labels_from_dynamic_gesture_csv(unpruned)\n",
    "    return extract_training_data_and_labels_from_dynamic_gesture_map(pruned)\n",
    "\n",
    "extracted_data,extracted_labels = extract_training_data_and_labels_from_dynamic_gesture_csv()\n",
    "len(extracted_data),len(extracted_labels)\n",
    "# map = training_trajectories_with_normalized_landmarks(gestures)\n",
    "# train_data, train_labels  = extract_training_data_and_labels_from_dynamic_gesture_map(map)\n",
    "# len(extracted_data),len(extracted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "# model_logistic = LogisticRegression()\n",
    "# model_logistic.fit(extracted_data, extracted_labels)\n",
    "\n",
    "\n",
    "model_svm = make_pipeline(StandardScaler(),\n",
    "                          SVC(kernel=\"poly\", degree=6, coef0=1))\n",
    "model_svm.fit(extracted_data, extracted_labels)\n",
    "\n",
    "if False:\n",
    "    from joblib import dump\n",
    "    dump(model_svm, 'dynamic_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_tuple_hand_landmarks_normalized(tuple_list:list[Tuple]):\n",
    "    random.seed(42)\n",
    "    res = [tuple_list[0]]\n",
    "    res.extend(random.sample(tuple_list[1:-1], bob.target_len - 2))\n",
    "    res.append(tuple_list[-1])\n",
    "    \n",
    "    \n",
    "    hand_landmark_seq = [ hand_landmark for hand_landmark, _ in res ]\n",
    "    flatmarks = [ flatmark for _, flatmark in res ] #without Z values\n",
    "    return hand_landmark_seq, flatmarks\n",
    "\n",
    "def mp_landmarks_to_dynamic_prediction_ready(landmarks):\n",
    "    tuple_list = []\n",
    "    for landmark in landmarks:\n",
    "        pre_processed = pre_process_landmark(calc_landmark_list(landmark))\n",
    "        hand_landmark = np.array(landmark)\n",
    "        tuple_list.append((hand_landmark, pre_processed))\n",
    "    \n",
    "    key_frame_tuple = sample_tuple_hand_landmarks_normalized(tuple_list)\n",
    "\n",
    "    flatmarks = key_frame_tuple[1]\n",
    "    sequence_as_np_array = np.array(key_frame_tuple[0])\n",
    "    trajectory = bob.make_trajectory(sequence_as_np_array)\n",
    "\n",
    "    xyz = []\n",
    "    for trajectory_elm in trajectory.directions:\n",
    "        xyz.append(trajectory_elm.x.value)\n",
    "        xyz.append(trajectory_elm.y.value)\n",
    "        xyz.append(trajectory_elm.z.value)\n",
    "    xyz = np.array(xyz)\n",
    "\n",
    "    \n",
    "    for flat_landmark in flatmarks:\n",
    "        xyz = np.append(xyz, flat_landmark)\n",
    "\n",
    "    return xyz\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test on a single Z\n",
    "TEST_PATH = '../backend/dynamic_signs/test'\n",
    "\n",
    "test_data = mp.process_dynamic_gestures_from_folder(TEST_PATH)\n",
    "#res = list(filter(lambda x : x.label == 'Z', res))\n",
    "labels_test = [result.label for result in test_data]\n",
    "labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_material = []\n",
    "for dynamic_gesture in test_data:\n",
    "    for sequence in dynamic_gesture.results:\n",
    "        hand_landmarks_for_seq = [mp_result.multi_hand_landmarks for mp_result in sequence \n",
    "                                  if mp_result.multi_hand_landmarks is not None]\n",
    "        prediction_material.append(\n",
    "            mp_landmarks_to_dynamic_prediction_ready(hand_landmarks_for_seq)\n",
    "        )\n",
    "\n",
    "print(\"Logistis predictions: \", model_logistic.predict(prediction_material))\n",
    "print(\"Support vector machine: \", model_svm.predict(prediction_material))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metrics for Dynamic Gesture recogniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "cr = classification_report(model_svm.predict(prediction_material), labels_test)\n",
    "print(cr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
